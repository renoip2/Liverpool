{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This kernel is a modified version of the 'üí•3 Simple Ideas - LB 0.938' kernel from [adrianoavelar](https://www.kaggle.com/adrianoavelar) \n# I have added an ensemble of models to improve CV/LB"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/18045/logos/header.png?t=2020-02-21-18-37-17\" width=1900px height=400px />"},{"metadata":{},"cell_type":"markdown","source":"# Ion Switching\n## Drift Removal + Feature Engineering + Overfitting (Train)\n\n<h3 style=\"color:red\"> If this Kernel Helped You! Please UP VOTE! üòÅ </h3>\n\n<h3> Kernel description: </h3>\n\n\nWhat I love most about Kaggle is the sharing of solutions. For me, it is far from a competition. Kaggle is the true meaning of research, that is, the combination of several ideas from different people to find the solution to a problem.\n\nThis Kernel uses ideas from me and other authors. The main ideas are the removal of the signal Drift, presented by @cdeotte, Feature Engineering from the work of @jazivxt and implementation of @ kmat2019 of a [discussion](https://www.kaggle.com/c/liverpool-ion-switching/discussion/133142) that proved that the first 30% of the test data is used to calculate public score. Below, the link to the mentioned notebooks.\n\n* Drift Removal: https://www.kaggle.com/cdeotte/one-feature-model-0-930\n* Feature Engineering: https://www.kaggle.com/jazivxt/physically-possible\n* Overfitting (Train): https://www.kaggle.com/kmat2019/train-test-similarity-analysis\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents:\n\n**1. [Problem Definition](#id1)** <br>\n**2. [Get the Data (Collect / Obtain)](#id2)** <br>\n**3. [Load the Dataset](#id3)** <br>\n**4. [Data Pre-processing](#id4)** <br>\n**5. [Model](#id5)** <br>\n**6. [Visualization and Analysis of Results](#id6)** <br>\n**7. [Submittion](#id7)** <br>\n**8. [References](#ref)** <br>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id1\"></a> <br> \n# **1. Problem Definition:** \n<img src=\"https://www.news-medical.net/image.axd?picture=2018%2F10%2Fshutterstock_480412786.jpg\" />\n\nMany diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants\nIf scientists could better study ion channels, it could have a far-reaching impact.\n\nIon channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction.\n\nIn this competition, you‚Äôll use ion channel data to better model automatic identification methods. If successful, you‚Äôll be able to detect individual ion channel events in noisy raw signals."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id2\"></a> <br> \n# **2. Get the Data (Collect / Obtain):** \n> Loading all libs and necessary datasets"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom sklearn import *\nfrom sklearn.metrics import f1_score\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import Pool,CatBoostRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport datetime\n\nsns.set_style(\"whitegrid\")\n\nfrom sklearn.model_selection import KFold\n\n#Constants\nROW_PER_BATCH = 500000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading data\ntrain = pd.read_csv('/kaggle/input/liverpool-ion-switching/train.csv')\ntest = pd.read_csv('/kaggle/input/liverpool-ion-switching/test.csv')\n\n\nprint('Shape of train is ',train.shape)\nprint('Shape of test is ',test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id3\"></a> <br> \n# **3. Understand the Data** "},{"metadata":{},"cell_type":"markdown","source":"While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['batch'] = 0\n\nfor i in range(0, train.shape[0]//ROW_PER_BATCH):\n    train.iloc[i * ROW_PER_BATCH: (i+1) * ROW_PER_BATCH,3] = i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(26, 22))\nplt.subplots_adjust(top=1.2, hspace = 0.8)\nfor i, b in enumerate(train['batch'].unique()):\n    plt.subplot(5, 2, i + 1)\n    plt.plot(train.loc[train['batch'] == b, ['signal']], color='b')\n    plt.title(f'Batch: {b}')\n    plt.plot(train.loc[train['batch'] == b, ['open_channels']], color='r')\n    plt.legend(['signal', 'open_channels'], loc=(0.875, 0.9))\n    plt.grid(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Findings\n\n1) batch 0: It looks like the most stable data part. Where you can clearly see the signs and open channels for correspondence. In this graph, open_channels are binary events with low probability<br>\n2) batch 1: There also appears to be a certain correlation, even though there is a shift in signal levels after the 600k time. In this graph, open_channels are binary events with LOW probability.<br>\n3) batch 2 and 6: open_channels are binary events with HIGH probability<br>\n**4) A Drift was added to batches 6, 7, 8, 9. And the beginning of batch 1.**"},{"metadata":{},"cell_type":"markdown","source":"## Interesting Pattern \n\nIt can be seen a patern batches, for every batch there is another with almost identical target distribution, same color here."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(26, 22))\ncolors = ['red','red','blue','pink','gold','brown','blue','pink','brown','gold']\nfor i, b in enumerate(train['batch'].unique()):\n    plt.subplot(5, 2, i + 1)\n    train.iloc[i*ROW_PER_BATCH : (i+1)*ROW_PER_BATCH]['open_channels'].value_counts().plot(kind='bar',color= colors[i])\n    plt.title(f'Batch: {b}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Chacing distribution\n\nJust checking the distribution to seek for "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\nplt.figure(figsize=(26, 22))\nplt.subplots_adjust(top=1.2, hspace = 0.8)\ncolors = ['red','red','blue','pink','gold','brown','blue','pink','brown','gold']\nfor i in train['batch'].unique():\n    df[ f'batch:{i}' ] = train.iloc[i*ROW_PER_BATCH : (i+1)*ROW_PER_BATCH].reset_index().signal \n    plt.subplot(5, 2, i + 1)\n    \n    sns.distplot(df[f'batch:{i}'], color= colors[i]).set_title(f\"(Signal) median: {df[f'batch:{i}'].median():.2f}\")\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\n\nsns.boxplot(x=\"variable\",y=\"value\",data=pd.melt(df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Data Analisys"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['batch'] = 0\n\nfor i in range(0, test.shape[0]//ROW_PER_BATCH):\n    test.iloc[i * ROW_PER_BATCH: (i+1) * ROW_PER_BATCH,2] = i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_signal = []\ndf = pd.DataFrame()\ntemplis = {}\nplt.figure(figsize=(26, 22))\nplt.subplots_adjust(top=1.2, hspace = 0.8)\ncolors = ['red','red','blue','pink','gold','brown','blue','pink','brown','gold']\nfor i in test['batch'].unique():\n    df[ f'batch:{i}' ] = test.iloc[i*ROW_PER_BATCH : (i+1)*ROW_PER_BATCH].reset_index().signal \n    plt.subplot(5, 2, i + 1)\n    sns.distplot(df[f'batch:{i}'], color= colors[i]).set_title(f\"(Signal) median: {df[f'batch:{i}'].median():.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxplot(x=\"variable\",y=\"value\",data=pd.melt(df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The idea here is identify some patterns and then create groups from these patterns to split the data. After that, we can make diferente models for each of patterns. For now, I'm not using this idea, but I will later."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['type'] = 0\nfor i in range(train['batch'].nunique()):\n    median = train.iloc[i*ROW_PER_BATCH : (i+1) * ROW_PER_BATCH].signal.median()\n    if (median < 0):\n        train.iloc[i*ROW_PER_BATCH : (i+1) * ROW_PER_BATCH, train.columns.get_loc('type')] = 0\n    else:\n        train.iloc[i*ROW_PER_BATCH : (i+1) * ROW_PER_BATCH, train.columns.get_loc('type')] = 1\n        \ntest['type'] = 0\n\nROW_PER_BATCH = 100000\n\nfor i in range(test['batch'].nunique()):\n    median = test.iloc[i*ROW_PER_BATCH : (i+1)*ROW_PER_BATCH].signal.median()\n    if (median < 0):\n        test.iloc[i*ROW_PER_BATCH : (i+1) * ROW_PER_BATCH, test.columns.get_loc('type')] = 0\n    else:\n        test.iloc[i*ROW_PER_BATCH : (i+1) * ROW_PER_BATCH, test.columns.get_loc('type')] = 1    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove Drift from Training Data\nfrom: https://www.kaggle.com/cdeotte/one-feature-model-0-930\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot( train.signal[500000:1000000][::100] )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking if this changes the data distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"a = 500000; b = a *2\nprint( 'Before: mean: {} std: {} median: {}'.format( train.signal[a:b].mean(), train.signal[a:b].std(),train.signal[a:b].median() ) )\n\na=500000; b=600000\ntrain['signal_undrifted'] = train.signal\ntrain.loc[train.index[a:b],'signal_undrifted'] = train.signal[a:b].values - 3*(train.time.values[a:b] - 50)/10.\n\na = 500000; b = a *2\nprint( 'After: mean: {} std: {} median: {}'.format( train.signal_undrifted[a:b].mean(), train.signal_undrifted[a:b].std(),train.signal_undrifted[a:b].median() ) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in figure below, the drift removal makes signal closer to a normal distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,6))\nsns.distplot(train.signal[500000:1000000],color='r')\nsns.distplot(train.signal_undrifted[500000:1000000],color='g' ).set(xlabel=\"Signal\")\nplt.legend(labels=['Original Signal','Undrifted Signal'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing Drift from Batch 7-10"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef f(x,low,high,mid): return -((-low+high)/625)*(x-mid)**2+high -low\n\n# CLEAN TRAIN BATCH 7\nbatch = 7; a = 500000*(batch-1); b = 500000*batch\ntrain.loc[train.index[a:b],'signal_undrifted'] = train.signal.values[a:b] - f(train.time[a:b].values,-1.817,3.186,325)\n# CLEAN TRAIN BATCH 8\nbatch = 8; a = 500000*(batch-1); b = 500000*batch\ntrain.loc[train.index[a:b],'signal_undrifted'] = train.signal.values[a:b] - f(train.time[a:b].values,-0.094,4.936,375)\n# CLEAN TRAIN BATCH 9\nbatch = 9; a = 500000*(batch-1); b = 500000*batch\ntrain.loc[train.index[a:b],'signal_undrifted'] = train.signal.values[a:b] - f(train.time[a:b].values,1.715,6.689,425)\n# CLEAN TRAIN BATCH 10\nbatch = 10; a = 500000*(batch-1); b = 500000*batch\ntrain.loc[train.index[a:b],'signal_undrifted'] = train.signal.values[a:b] - f(train.time[a:b].values,3.361,8.45,475)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nsns.lineplot(train.time[::1000],train.signal[::2000],color='r').set_title('Training Batches 7-10 with Parabolic Drift')\n#plt.figure(figsize=(20,5))\ng = sns.lineplot(train.time[::1000],train.signal_undrifted[::2000],color='g').set_title('Training Batches 7-10 without Parabolic Drift')\nplt.legend(title='Train Data',loc='upper left', labels=['Original Signal', 'UnDrifted Signal'])\nplt.show(g)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['signal_undrifted'] = test.signal\n\n# REMOVE BATCH 1 DRIFT\nstart=500\na = 0; b = 100000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - 3*(test.time.values[a:b]-start)/10.\nstart=510\na = 100000; b = 200000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - 3*(test.time.values[a:b]-start)/10.\nstart=540\na = 400000; b = 500000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - 3*(test.time.values[a:b]-start)/10.\n\n# REMOVE BATCH 2 DRIFT\nstart=560\na = 600000; b = 700000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - 3*(test.time.values[a:b]-start)/10.\nstart=570\na = 700000; b = 800000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - 3*(test.time.values[a:b]-start)/10.\nstart=580\na = 800000; b = 900000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - 3*(test.time.values[a:b]-start)/10.\n\n# REMOVE BATCH 3 DRIFT\ndef f(x):\n    return -(0.00788)*(x-625)**2+2.345 +2.58\na = 1000000; b = 1500000\ntest.loc[test.index[a:b],'signal_undrifted'] = test.signal.values[a:b] - f(test.time[a:b].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove Drift from Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nsns.lineplot(test.time[::1000],test.signal[::1000],color='r').set_title('Training Batches 7-10 with Parabolic Drift')\n#plt.figure(figsize=(20,5))\ng = sns.lineplot(test.time[::1000],test.signal_undrifted[::1000],color='g').set_title('Training Batches 7-10 without Parabolic Drift')\nplt.legend(title='Test Data',loc='upper right', labels=['Original Signal', 'UnDrifted Signal'])\nplt.show(g)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id5\"></a> <br> \n# **5. Feature Engineering** "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index // 50_000\n    df['batch_index'] = df.index  - (df.batch * 50_000)\n    df['batch_slices'] = df['batch_index']  // 5_000\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in ['batch','batch_slices2']:\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal_undrifted'].mean()\n        d['median'+c] = df.groupby([c])['signal_undrifted'].median()\n        d['max'+c] = df.groupby([c])['signal_undrifted'].max()\n        d['min'+c] = df.groupby([c])['signal_undrifted'].min()\n        d['std'+c] = df.groupby([c])['signal_undrifted'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal_undrifted'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal_undrifted'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal_undrifted'].apply(lambda x: np.min(np.abs(x)))\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n        df['range'+c] = df['max'+c] - df['min'+c]\n        df['maxtomin'+c] = df['max'+c] / df['min'+c]\n        df['abs_avg'+c] = (df['abs_min'+c] + df['abs_max'+c]) / 2\n    \n    #add shifts\n    df['signal_shift_+1'] = [0,] + list(df['signal_undrifted'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal_undrifted'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n\n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal_undrifted', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]:\n        df[c+'_msignal'] = df[c] - df['signal_undrifted']\n        \n    return df\n\ntrain = features(train)\ntest = features(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id5\"></a> <br> \n# **6. Model**\n\n> Simple Model: It's can be improved."},{"metadata":{"trusted":true},"cell_type":"code","source":"def f1_score_calc(y_true, y_pred):\n    return f1_score(y_true, y_pred, average=\"macro\")\n\ndef lgb_Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average=\"macro\")\n    return ('KaggleMetric', score, True)\n\n\ndef train_model_classification(X, X_test, y, params, model_type='lgb', eval_metric='f1score',\n                               columns=None, plot_feature_importance=False, model=None,\n                               verbose=50, early_stopping_rounds=200, n_estimators=2000):\n\n    columns = X.columns if columns == None else columns\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {\n                    'f1score': {'lgb_metric_name': lgb_Metric,}\n                   }\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X) )\n    \n    # averaged predictions on train data\n    prediction = np.zeros((len(X_test)))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    '''for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]'''\n            \n    if True:        \n        X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.3, random_state=7)    \n            \n        if model_type == 'lgb':\n            #model = lgb.LGBMClassifier(**params, n_estimators=n_estimators)\n            #model.fit(X_train, y_train, \n            #        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n            #       verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            model = lgb.train(params, lgb.Dataset(X_train, y_train),\n                              n_estimators,  lgb.Dataset(X_valid, y_valid),\n                              verbose_eval=verbose, early_stopping_rounds=early_stopping_rounds, feval=lgb_Metric)\n            \n            \n            preds = model.predict(X, num_iteration=model.best_iteration) #model.predict(X_valid) \n\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            \n        if model_type == 'xgb':\n            train_set = xgb.DMatrix(X_train, y_train)\n            val_set = xgb.DMatrix(X_valid, y_valid)\n            model = xgb.train(params, train_set, num_boost_round=2222, evals=[(train_set, 'train'), (val_set, 'val')], \n                                     verbose_eval=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            preds = model.predict(xgb.DMatrix(X)) \n\n            y_pred = model.predict(xgb.DMatrix(X_test))\n            \n\n        if model_type == 'cat':\n            # Initialize CatBoostRegressor\n            model = CatBoostRegressor(params)\n            # Fit model\n            model.fit(X_train, y_train)\n            # Get predictions\n            y_pred_valid = np.round(np.clip(preds, 0, 10)).astype(int)\n\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            y_pred = np.round(np.clip(y_pred, 0, 10)).astype(int)\n\n \n        oof = preds\n        \n        scores.append(f1_score_calc(y, np.round(np.clip(preds,0,10)).astype(int) ) )\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    #prediction /= folds.n_splits\n    \n    print('FINAL score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    result_dict['model'] = model\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= folds.n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's uses 'singal_undrifted' instead of 'signal'\ngood_columns = [c for c in train.columns if c not in ['time', 'signal','open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]\n\nX = train[good_columns].copy()\ny = train['open_channels']\nX_test = test[good_columns].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_xgb = {'colsample_bytree': 0.375,'learning_rate': 0.1,'max_depth': 10, 'subsample': 1, 'objective':'reg:squarederror',\n          'eval_metric':'rmse'}\n\nresult_dict_xgb = train_model_classification(X=X[0:500000*8-1], X_test=X_test, y=y[0:500000*8-1], params=params_xgb, model_type='xgb', eval_metric='f1score', plot_feature_importance=False,\n                                                      verbose=50, early_stopping_rounds=250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#params_cat = {'task_type': \"CPU\",'iterations':1000,'learning_rate':0.1,'random_seed': 42,'depth':2}\n\n#result_dict_cat = train_model_classification(X=X[0:500000*8-1], X_test=X_test, y=y[0:500000*8-1], params=params_cat, model_type='cat', eval_metric='f1score', plot_feature_importance=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_lgb = {'learning_rate': 0.1, 'max_depth': 7, 'num_leaves':2**7+1, 'metric': 'rmse', 'random_state': 7, 'n_jobs':-1}\n\nresult_dict_lgb = train_model_classification(X=X[0:500000*8-1], X_test=X_test, y=y[0:500000*8-1], params=params_lgb, model_type='lgb', eval_metric='f1score', plot_feature_importance=False,\n                                                      verbose=50, early_stopping_rounds=250, n_estimators=3000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id5\"></a><br> \n# **7. Results**"},{"metadata":{},"cell_type":"markdown","source":"The model can score 0.938 on LB without further optimization. Maybe with some GridSearch for parameters tunning or more features engineering, it's possible to get to 0.94"},{"metadata":{"trusted":true},"cell_type":"code","source":"booster = result_dict_lgb['model']\n\nfi = pd.DataFrame()\nfi['importance'] = booster.feature_importance(importance_type='gain')\nfi['feature'] = booster.feature_name()\n\nbest_features = fi.sort_values(by='importance', ascending=False)[:20]\n\n\nplt.figure(figsize=(16, 12));\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features);\nplt.title('LGB Features (avg over folds)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id7\"></a> <br> \n# **8. Submittion** "},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_ensemble = 0.50 * result_dict_lgb['prediction'] + 0.50 * result_dict_xgb['prediction']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv')\nsub['open_channels'] =  np.array(np.round(preds_ensemble,0), np.int) \n\nsub.to_csv('submission_unshifted_70p.csv', index=False, float_format='%.4f')\nsub.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ref\"></a>\n# **8. References** \n\n[1] Deep-Channel uses deep neural networks to detect\nsingle-molecule events from patch-clamp data https://www.nature.com/articles/s42003-019-0729-3.pdf\n\n[2] The Patch Clamp Method: https://www.youtube.com/watch?v=mVbkSD5FHOw\n\n[3] Electophysiology: Patch clamp method https://www.youtube.com/watch?v=CvfXjGVNbmw\n\n[4] The Action Potential https://www.youtube.com/watch?v=HYLyhXRp298\n\n[5] https://www.kaggle.com/pestipeti/eda-ion-switching\n\n[6] https://www.kaggle.com/kmat2019/u-net-1d-cnn-with-keras\n\n[7] https://www.kaggle.com/cdeotte/one-feature-model-0-930"},{"metadata":{},"cell_type":"markdown","source":"> **Nota**: If you fork my work, please, upvote and give the credits ‚ò∫ <br>\nMade by[@AdrianoAvelar](https://www.kaggle.com/adrianoavelar) <br>\nFork and Code.\n<h3 style=\"color:red\"> If this Kernel Helped You! Please UP VOTE! üòÅ </h3>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}